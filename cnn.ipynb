{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"DATA/TRAIN_1_IRREGULAR_STEPS_V2.json\"\n",
    "dataset_path_2=\"DATA/TRAIN_2_IRREGULAR_STEPS.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from ManeuverDetectionDataset import ManeuverDetectionDataset, IrregularDataset, SlidingWindowDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_l_out(l_in, kernel_size, padding=0, dilation=1, stride=1):\n",
    "    return np.floor((l_in + 2 * padding - dilation * (kernel_size - 1) -1)/stride + 1)\n",
    "\n",
    "class ConvBlock1d(torch.nn.Module):\n",
    "    def __init__(self, conv_kwargs, pool_kwargs, dropout_rate) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        self.conv = torch.nn.Conv1d(**conv_kwargs)\n",
    "        self.activation_fn = torch.nn.ReLU()\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html\n",
    "        self.pooling = torch.nn.MaxPool1d(**pool_kwargs)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Cnn1d(torch.nn.Module):\n",
    "    def __init__(self, block_kwargs_list, linear_kwargs) -> None: # use_dv_head=False, use_date_head=False\n",
    "        super().__init__()\n",
    "        self.block_kwargs_list = block_kwargs_list\n",
    "        ll = []\n",
    "        for block_kwargs in block_kwargs_list:\n",
    "            ll.append(ConvBlock1d(**block_kwargs))\n",
    "\n",
    "        self.convnet = torch.nn.Sequential(*ll)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fcnn = torch.nn.Linear(**linear_kwargs)\n",
    "        self.activation_fn = torch.nn.ReLU()\n",
    "\n",
    "        self.classification_head = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(linear_kwargs['out_features'], 1),\n",
    "            # torch.nn.Softmax(dim=-1)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # main model\n",
    "        x = self.convnet(x)\n",
    "        embedding = torch.flatten(x, start_dim=1) # size : batch size x length\n",
    "        x = self.fcnn(embedding)\n",
    "        x = self.activation_fn(x)\n",
    "\n",
    "        # classification head\n",
    "        c = self.classification_head(x)\n",
    "        return torch.squeeze(c), embedding\n",
    "    \n",
    "    # def predict(self, x, return_embedding=False):\n",
    "    #     self.eval()\n",
    "    #     # main model\n",
    "    #     x = self.convnet(x)\n",
    "    #     embedding = torch.flatten(x, start_dim=1) # size : batch size x length\n",
    "    #     x = self.fcnn(embedding)\n",
    "    #     x = self.activation_fn(x)\n",
    "\n",
    "    #     # classification head\n",
    "    #     c = self.classification_head(x)\n",
    "    #     if(return_embedding):\n",
    "    #         return c, embedding\n",
    "    #     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class TorchTrainer:\n",
    "    def __init__(self, model, verbose=True, weight=None, index_y=0, loss_function=\"BCELoss\") -> None:\n",
    "        self.model = model\n",
    "        self.weight = weight\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        if(loss_function == 'CrossEntropyLoss'): # DO NOT USE IT\n",
    "            self.loss_function = torch.nn.CrossEntropyLoss(weight=self.weight) # torch.tensor([0.05, 0.95]))\n",
    "        elif(loss_function == \"L1Loss\"):\n",
    "            self.loss_function = torch.nn.L1Loss()\n",
    "        elif(loss_function == 'BCELoss'):\n",
    "            loss_function =  torch.nn.BCEWithLogitsLoss(weight=self.weight)\n",
    "            self.loss_function = lambda pred, target: loss_function(pred, target.to(torch.float))\n",
    "        self.optimiser = torch.optim.SGD(self.model.parameters(), lr=1e-3, momentum=0.9)\n",
    "        self.verbose = verbose\n",
    "        self.index_y = index_y\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr):\n",
    "        train_loss_list = []\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimiser, base_lr=lr/2., max_lr=2 * lr)\n",
    "        t = tqdm(range(epochs), desc='0 - loss: 0', leave=True, disable=not self.verbose)\n",
    "        for epoch in t:\n",
    "            train_loss = self.train_one_epoch(train_loader, scheduler)\n",
    "            train_loss_list.append(train_loss)\n",
    "            t.set_description(\"{} - loss: {:0.2f}\".format(epoch, train_loss), refresh=True)\n",
    "        return train_loss_list\n",
    "\n",
    "    def train_one_epoch(self, train_loader, scheduler):\n",
    "        loss_list = []\n",
    "        for x, y in train_loader:\n",
    "            self.optimiser.zero_grad()\n",
    "            x = x.to(self.device)\n",
    "            y = tuple([y_.to(self.device) for y_ in y])\n",
    "            pred, embedding = self.model(x)\n",
    "            loss = self.loss_function(pred, y[self.index_y])\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "            scheduler.step()\n",
    "\n",
    "        return np.mean(loss_list)\n",
    "\n",
    "    def predict(self, test_loader, return_true=False):\n",
    "        self.model.eval()\n",
    "        c_pred_list = []\n",
    "        c_true_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = tuple([y_.to(self.device) for y_ in y])\n",
    "                pred = self.model(x)\n",
    "                c_pred_list.append(pred)\n",
    "                # dv_pred_list.append(pred[1])\n",
    "                c_true_list.append(y[self.index_y])\n",
    "                # dv_true_list.append(y[1])\n",
    "        self.model.train()\n",
    "\n",
    "        pred_tuple = (torch.concatenate(c_pred_list, axis=0),)\n",
    "                # torch.concatenate(dv_pred_list, axis=0))        \n",
    "        if(return_true):\n",
    "            true_tuple = (torch.concatenate(c_true_list, axis=0),)\n",
    "                # torch.concatenate(dv_true_list, axis=0))\n",
    "            return true_tuple, pred_tuple\n",
    "        return pred_tuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 :\n",
    "Detect which time series contain maneuvers.\n",
    "\n",
    "CNN to determine on full time series, not evenly spaced, if it contains a maneuver or not.\n",
    "\n",
    "Fixed size of the time series : 1000 (48h of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_kwargs_list_1000 = [\n",
    "    { # layer 1\n",
    "        'conv_kwargs': {\n",
    "            'in_channels': 3,\n",
    "            'out_channels': 4,\n",
    "            'kernel_size': 7,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            'dilation': 1,\n",
    "            'groups': 1,\n",
    "            'bias': True,\n",
    "            'padding_mode': 'zeros'\n",
    "        },\n",
    "        'pool_kwargs': {\n",
    "            'kernel_size': 7,\n",
    "            'stride': None,\n",
    "            'padding': 0,\n",
    "            'dilation': 1\n",
    "        },\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    { # layer 2\n",
    "        'conv_kwargs': {\n",
    "            'in_channels': 4,\n",
    "            'out_channels': 6,\n",
    "            'kernel_size': 7,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            'dilation': 1,\n",
    "            'groups': 1,\n",
    "            'bias': True,\n",
    "            'padding_mode': 'zeros'\n",
    "        },\n",
    "        'pool_kwargs': {\n",
    "            'kernel_size': 7,\n",
    "            'stride': None,\n",
    "            'padding': 0,\n",
    "            'dilation': 1\n",
    "        },\n",
    "        'dropout_rate': 0.0\n",
    "    },\n",
    "    { # layer 3\n",
    "        'conv_kwargs': {\n",
    "            'in_channels': 6,\n",
    "            'out_channels':6,\n",
    "            'kernel_size': 5,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            'dilation': 1,\n",
    "            'groups': 1,\n",
    "            'bias': True,\n",
    "            'padding_mode': 'zeros'\n",
    "        },\n",
    "        'pool_kwargs': {\n",
    "            'kernel_size': 5,\n",
    "            'stride': None,\n",
    "            'padding': 0,\n",
    "            'dilation': 1\n",
    "        },\n",
    "        'dropout_rate': 0.0\n",
    "    }\n",
    "]\n",
    "linear_kwargs_1000 = {\n",
    "    'in_features': 18,\n",
    "    'out_features': 10 # size of the projection space (dimension reduction)\n",
    "}\n",
    "\n",
    "conv_net_1000 = Cnn1d(block_kwargs_list_1000, linear_kwargs_1000).float()\n",
    "\n",
    "# test\n",
    "c = conv_net_1000(torch.zeros(4, 3, 1000).float())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = IrregularDataset([\n",
    "    ManeuverDetectionDataset(dataset_path, dataset_type=\"TRAIN\"),\n",
    "    ManeuverDetectionDataset(dataset_path_2, dataset_type=\"TRAIN\")])\n",
    "valid_dataset = IrregularDataset([\n",
    "    ManeuverDetectionDataset(dataset_path, dataset_type=\"VALIDATION\"),\n",
    "    ManeuverDetectionDataset(dataset_path_2, dataset_type=\"VALIDATION\")])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, drop_last=True, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, drop_last=True, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcnn import TCNModel\n",
    "tcn_model = TCNModel(num_channels=[20, 20]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_tcnn = TorchTrainer(model=tcn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-3, 5e-4, 5e-3]: # 5e-3, 2e-3, \n",
    "    print(trainer_tcnn.train(train_loader, epochs=7, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_valid, pred_valid = trainer_tcnn.predict(valid_loader, return_true=True)\n",
    "true_train, pred_train = trainer_tcnn.predict(train_loader, return_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.count_nonzero((pred_valid[0] > 0.5) == true_valid[0])/true_valid[0].shape[0])\n",
    "print(torch.count_nonzero((pred_train[0] > 0.5) == true_train[0])/true_train[0].shape[0])\n",
    "\n",
    "y_true, y_pred = true_train[0].cpu().numpy(), (pred_train[0]>0.5).cpu().numpy()\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "y_true, y_pred = true_valid[0].cpu().numpy(), (pred_valid[0]>0.5).cpu().numpy()\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net_1000 = Cnn1d(block_kwargs_list_1000, linear_kwargs_1000).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(model=conv_net_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-3, 5e-4]: # 5e-3, 2e-3,  2e-4, 1e-4, 5e-4\n",
    "    print(trainer.train(train_loader, epochs=4, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_valid, pred_valid = trainer.predict(valid_loader, return_true=True)\n",
    "true_train, pred_train = trainer.predict(train_loader, return_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.count_nonzero(pred_valid[0] > 0.5 == true_valid[0])/true_valid[0].shape[0])\n",
    "print(torch.count_nonzero(pred_train[0] > 0.5 == true_train[0])/true_train[0].shape[0])\n",
    "\n",
    "y_true, y_pred = true_train[0].cpu().numpy(), (pred_train[0]>0.5).cpu().numpy()\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "y_true, y_pred = true_valid[0].cpu().numpy(), (pred_valid[0]>0.5).cpu().numpy()\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Step - First solution\n",
    "\n",
    "Now that we have the problematric ones, we can try to determine both the dV and the time of the maneuver.\n",
    "How do we do ?\n",
    "\n",
    "We use a simple linear model on the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset_man_only = IrregularDataset([\n",
    "    ManeuverDetectionDataset(dataset_path, dataset_type=\"TRAIN\", filter_samples='MANEUVER_ONLY'),\n",
    "    ManeuverDetectionDataset(dataset_path_2, dataset_type=\"TRAIN\", filter_samples='MANEUVER_ONLY')])\n",
    "valid_dataset_man_only = IrregularDataset([\n",
    "    ManeuverDetectionDataset(dataset_path, dataset_type=\"VALIDATION\", filter_samples='MANEUVER_ONLY'),\n",
    "    ManeuverDetectionDataset(dataset_path_2, dataset_type=\"VALIDATION\", filter_samples='MANEUVER_ONLY')])\n",
    "\n",
    "train_loader_man_only = DataLoader(train_dataset_man_only, batch_size=8, drop_last=False, shuffle=True)\n",
    "valid_loader_man_only = DataLoader(valid_dataset_man_only, batch_size=8, drop_last=False, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heads & Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManeuverTimeHead(torch.nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features, out_features=5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(5, out_features=1)\n",
    "        tanh = torch.nn.Tanh()\n",
    "        self.output_fn = lambda x : 0.5 * (1.0 + tanh(x)) # torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        x = self.linear1(embedding)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return torch.squeeze(self.output_fn(x))\n",
    "\n",
    "class DeltaVelocityHead(torch.nn.Module):\n",
    "    def __init__(self, in_features) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features, out_features=5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(5, out_features=1)\n",
    "        tanh = torch.nn.Tanh()\n",
    "        self.output_fn = lambda x : 0.5 * (1.0 + tanh(x)) # torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        x = self.linear1(embedding)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return torch.squeeze(self.output_fn(x)) # Dv max is like 1.5 in absolute value\n",
    "    \n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, convnet, model_to_train)  -> None:\n",
    "        super().__init__()\n",
    "        self.convnet = convnet\n",
    "        self.model_to_train = model_to_train\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c, embedding = self.convnet(x) # NOTE : we suppose we only send in data with manoeuver in it\n",
    "        output = self.model_to_train(embedding) \n",
    "        return output, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maneuver_time_net = ManeuverTimeHead(linear_kwargs_1000['in_features'])\n",
    "dv_net = DeltaVelocityHead(linear_kwargs_1000['in_features'])\n",
    "\n",
    "# freeze network\n",
    "for param in conv_net_1000.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "time_net_wrapper = Wrapper(conv_net_1000, maneuver_time_net)\n",
    "dv_net_wrapper = Wrapper(conv_net_1000, dv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_trainer = TorchTrainer(model=time_net_wrapper, index_y=2, loss_function='L1Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [2e-3, 1e-3, 5e-4]: # 2e-3, 1e-3, \n",
    "    print(time_trainer.train(train_loader_man_only, epochs=5, lr=lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_trainer = TorchTrainer(model=dv_net_wrapper, index_y=1, loss_function='L1Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [2e-3, 1e-3, 5e-4]:\n",
    "    print(dv_trainer.train(train_loader_man_only, epochs=5, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_time, pred_time = time_trainer.predict(valid_loader_man_only, return_true=True)\n",
    "true_dv, pred_dv = dv_trainer.predict(valid_loader_man_only, return_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = torch.nn.L1Loss()\n",
    "print(l1_loss(true_time[0], pred_time[0]))\n",
    "print(l1_loss(true_dv[0], pred_dv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(true_time[0].cpu().numpy(), pred_time[0].cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Training the cnn too ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net_100_time = Cnn1d(block_kwargs_list_1000, linear_kwargs_1000).float()\n",
    "conv_net_100_dv = Cnn1d(block_kwargs_list_1000, linear_kwargs_1000).float()\n",
    "conv_net_100_time.load_state_dict(conv_net_1000.state_dict())\n",
    "conv_net_100_dv.load_state_dict(conv_net_1000.state_dict())\n",
    "time_net_wrapper_bis = Wrapper(conv_net_100_time, maneuver_time_net)\n",
    "dv_net_wrapper_bis = Wrapper(conv_net_100_dv, dv_net)\n",
    "\n",
    "for param in time_net_wrapper_bis.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in dv_net_wrapper_bis.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_trainer_bis = TorchTrainer(model=time_net_wrapper_bis, index_y=2, loss_function='L1Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [2e-3, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5]:\n",
    "    print(time_trainer_bis.train(train_loader_man_only, epochs=3, lr=lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_trainer_bis = TorchTrainer(model=dv_net_wrapper_bis, index_y=1, loss_function='L1Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [2e-3, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5]:\n",
    "    print(dv_trainer_bis.train(train_loader_man_only, epochs=3, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_time, pred_time = time_trainer_bis.predict(valid_loader_man_only, return_true=True)\n",
    "true_dv, pred_dv = dv_trainer_bis.predict(valid_loader_man_only, return_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = torch.nn.L1Loss()\n",
    "print(l1_loss(true_time[0], pred_time[0]))\n",
    "print(l1_loss(true_dv[0], pred_dv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(true_time[0].cpu().numpy(), pred_time[0].cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InferenceWrapper(torch.nn.Module):\n",
    "    def __init__(self, convnet, dv_net, time_net) -> None:\n",
    "        super().__init__()\n",
    "        self.convnet = convnet\n",
    "        self.dv_net = dv_net\n",
    "        self.time_net = time_net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c, embedding = self.convnet.predict(x)\n",
    "        dv = self.dv_net(embedding) \n",
    "        time = self.time_net(embedding)\n",
    "        return (c, dv, time), embedding\n",
    "    \n",
    "    def predict(self, dataloader, use_convnet_embedding=False):\n",
    "        self.eval()\n",
    "        cc_list = []\n",
    "        time_list = []\n",
    "        time_true = []\n",
    "        dv_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                c, embedding = self.convnet.predict(x, return_embedding=True)\n",
    "                # cc = torch.argmax(c, dim=-1)\n",
    "                # cache = cc == 1\n",
    "                # dv = torch.zeros(cc.shape)\n",
    "                # time = torch.zeros(cc.shape)\n",
    "                if(use_convnet_embedding):\n",
    "                    dv = self.dv_net(embedding)\n",
    "                    time = self.time_net(embedding)\n",
    "                else:\n",
    "                    dv = self.dv_net(x)\n",
    "                    time = self.time_net(x)\n",
    "                cc_list.append(cc.cpu().numpy())\n",
    "                time_list.append(time.cpu().numpy())\n",
    "                time_true.append(y[2].cpu().numpy())\n",
    "                dv_list.append(dv.cpu().numpy())\n",
    "        return ((np.concatenate(cc_list),\n",
    "                np.array(dv_list).flatten(),\n",
    "                np.array(time_list).flatten()), \n",
    "                (np.array(time_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path=\"DATA/TEST_FILE_PUBLIC.json\"\n",
    "test_dataset= ManeuverDetectionDataset(test_dataset_path, dataset_type=\"TEST\")\n",
    "test_dataset_irr = IrregularDataset([ManeuverDetectionDataset(dataset_path, dataset_type=\"TEST\", imported_dataset=test_dataset.dataset)])\n",
    "test_loader = DataLoader(test_dataset_irr, batch_size=1, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "import pickle\n",
    "file_name = \"xgb_reg.pkl\"\n",
    "# load\n",
    "xgb_model_loaded = pickle.load(open(file_name, \"rb\"))\n",
    "\n",
    "with np.load('interpolated_ra_test.npz') as data:\n",
    "    x_scaled = data['x_scaled']\n",
    "    test_is_maneuver = data['is_maneuver']\n",
    "    test_feature_ra = data['feature_ra']\n",
    "    test_x_scaler = data['x_scaler']\n",
    "    test_y_scalers = data['y_scalers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classification = xgb_model_loaded.predict(test_feature_ra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze network\n",
    "\n",
    "inference_wrapper = InferenceWrapper(\n",
    "        convnet=conv_net_1000, # which one ???\n",
    "        dv_net=dv_net_wrapper_bis,\n",
    "        time_net=time_net_wrapper_bis\n",
    ")\n",
    "for param in inference_wrapper.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, true = inference_wrapper.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.stack(preds, axis=1) \n",
    "# print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,2]=48*3600*pred[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:, 1] = pred_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SubmissionGenerator import create_submission\n",
    "import numpy as np\n",
    "# pred[:,1]=0.01*np.ones((len(test_dataset))) #dv\n",
    "create_submission(pred,\"DATA/vg1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second step - Second Solution : for those we detected an anomaly for\n",
    "We split them in several subseries (sliding windows) and we have to find where the anomaly start occuring. \n",
    "For each small window, we determine a new score of anomaly. We can refine as many time as required.\n",
    "\n",
    "May be now we can try predicting the time of the anomaly occuring. I am not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ManeuverDetectionDataset import ManeuverDetectionDataset, ManeuverDetectionSlidingWindowDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "evenly_spaced_dataset_path=\"DATA/TRAIN_1_EVENLY_SPACED_V2.json\"\n",
    "evenly_spaced_dataset= ManeuverDetectionDataset(evenly_spaced_dataset_path, fixed_step=True) # window_size=30)\n",
    "evenly_spaced_dataset_sliding_window = ManeuverDetectionSlidingWindowDataset(evenly_spaced_dataset, window_size=433)\n",
    "# feature,is_maneuver,maneuver_dv,maneuver_time =next(iter(evenly_spaced_loader))\n",
    "# print(f\"features shape (batch size * nb of meas * nb of feature):{feature.shape}\\nis maneuver: {is_maneuver.item()}\\ndv (m/s): {maneuver_dv.item()}\\nmaneuver date (seconds from the observation start): {maneuver_time.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenly_spaced_loader = DataLoader(evenly_spaced_dataset_sliding_window, batch_size=32, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(evenly_spaced_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenly_spaced_dataset_valid = ManeuverDetectionDataset(evenly_spaced_dataset_path, dataset_type='VALIDATION', fixed_step=True)\n",
    "valid_set = ManeuverDetectionSlidingWindowDataset(evenly_spaced_dataset_valid, window_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(evenly_spaced_dataset_sliding_window, batch_size=1, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true, pred = trainer.predict(valid_loader, return_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.count_nonzero((pred[0] > 0.5) == true[0])/true[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_true, y_pred = true[0].cpu().numpy(), torch.argmax(pred[0], dim=1).cpu().numpy()\n",
    "metrics.confusion_matrix(y_true, y_pred) # problem with those definitely - class are completly UNBALANCED. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_kwargs_list_30 = [\n",
    "    { # layer 1\n",
    "        'conv_kwargs': {\n",
    "            'in_channels': 2,\n",
    "            'out_channels': 4,\n",
    "            'kernel_size': 3,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            'dilation': 1,\n",
    "            'groups': 1,\n",
    "            'bias': True,\n",
    "            'padding_mode': 'zeros'\n",
    "        },\n",
    "        'pool_kwargs': {\n",
    "            'kernel_size': 3,\n",
    "            'stride': None,\n",
    "            'padding': 0,\n",
    "            'dilation': 1\n",
    "        },\n",
    "        'dropout_rate': 0\n",
    "    },\n",
    "    { # layer 2\n",
    "        'conv_kwargs': {\n",
    "            'in_channels': 4,\n",
    "            'out_channels': 4,\n",
    "            'kernel_size': 3,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            'dilation': 1,\n",
    "            'groups': 1,\n",
    "            'bias': True,\n",
    "            'padding_mode': 'zeros'\n",
    "        },\n",
    "        'pool_kwargs': {\n",
    "            'kernel_size': 3,\n",
    "            'stride': None,\n",
    "            'padding': 0,\n",
    "            'dilation': 1\n",
    "        },\n",
    "        'dropout_rate': 0\n",
    "    }\n",
    "]\n",
    "linear_kwargs_30 = {\n",
    "    'in_features': 8,\n",
    "    'out_features': 10 # size of the projection space (dimension reduction)\n",
    "}\n",
    "conv_net_30 = Cnn1d(block_kwargs_list_30, linear_kwargs_30).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19916/536388 * 100 # 3.72% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss() # weight=torch.tensor([0.05, 0.95]))\n",
    "mae_loss_1 = torch.nn.L1Loss()\n",
    "mae_loss_2 = torch.nn.L1Loss()\n",
    "def total_loss_function(pred, true, alpha=0.5, beta=0.1, gamma=0.4):\n",
    "    c_true, dv_true, date_true = true\n",
    "    c, dv, date = pred\n",
    "    return alpha * cross_entropy_loss(c, c_true) + beta * mae_loss_1(dv, dv_true) + gamma * mae_loss_2(date_true, date)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98e39df5b3c61a6cccd34f8912856fa306a2dda7557d908626d35ee2df110a90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
